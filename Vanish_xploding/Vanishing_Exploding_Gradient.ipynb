{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "C64sKuGa9qh8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEL1VsYz9n8B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Neural Network"
      ],
      "metadata": {
        "id": "-CKRZCe89x_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNN(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.fc1=nn.Linear(784,256)\n",
        "        self.fc2=nn.Linear(256,128)\n",
        "        self.fc3=nn.Linear(128,64)\n",
        "        self.fc4=nn.Linear(64,32)\n",
        "        self.fc5=nn.Linear(32,10)\n",
        "  def forward(self,x):\n",
        "        out=torch.sigmoid(self.fc1(x))\n",
        "        out=torch.sigmoid(self.fc2(out))\n",
        "        out=torch.sigmoid(self.fc3(out))\n",
        "        out=torch.sigmoid(self.fc4(out))\n",
        "        out=torch.sigmoid(self.fc5(out))\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "7eczHDUR9wFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G37LU59Y_qps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Hyperparameters and generate dummy data\n",
        "\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "batch_size = 64\n",
        "x = torch.randn(batch_size, input_size)\n",
        "y = torch.randn(batch_size, output_size)\n"
      ],
      "metadata": {
        "id": "e9c12Sjc_o9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "deep_NN = DeepNN()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(deep_NN.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = deep_NN(x)\n",
        "    if outputs is not None:\n",
        "        loss = criterion(outputs, torch.argmax(y, dim=1))\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Print gradients at every 100 epochs\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Loss: {:.3f}'.format(epoch + 1, num_epochs, loss.item()))\n",
        "            \n",
        "\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9DweWf2PetH",
        "outputId": "9b7b3b5f-774a-45c0-e25e-90e5631b5726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 2.306\n",
            "Epoch [200/1000], Loss: 2.286\n",
            "Epoch [300/1000], Loss: 2.269\n",
            "Epoch [400/1000], Loss: 2.255\n",
            "Epoch [500/1000], Loss: 2.244\n",
            "Epoch [600/1000], Loss: 2.236\n",
            "Epoch [700/1000], Loss: 2.229\n",
            "Epoch [800/1000], Loss: 2.223\n",
            "Epoch [900/1000], Loss: 2.218\n",
            "Epoch [1000/1000], Loss: 2.214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As way to find solution for vanishing gradient there are different approaches one of which is Weight Initialization: Initialize the weights of the neural network with appropriate values to mitigate the vanishing gradient problem. One common technique is to use weight initialization methods such as Xavier or He initialization."
      ],
      "metadata": {
        "id": "TJu7FboVR5aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n"
      ],
      "metadata": {
        "id": "4ZU-fzacPqaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_NN.apply(init_weights)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(deep_NN.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = deep_NN(x)\n",
        "    if outputs is not None:\n",
        "        loss = criterion(outputs, torch.argmax(y, dim=1))\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Print gradients at every 100 epochs\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
        "            \n",
        "\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXf6nT4fSJcl",
        "outputId": "2543e5c4-06f5-49b2-9d69-bd7f2e9b8937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 2.2810\n",
            "Epoch [200/1000], Loss: 2.2626\n",
            "Epoch [300/1000], Loss: 2.2488\n",
            "Epoch [400/1000], Loss: 2.2382\n",
            "Epoch [500/1000], Loss: 2.2298\n",
            "Epoch [600/1000], Loss: 2.2230\n",
            "Epoch [700/1000], Loss: 2.2173\n",
            "Epoch [800/1000], Loss: 2.2125\n",
            "Epoch [900/1000], Loss: 2.2084\n",
            "Epoch [1000/1000], Loss: 2.2049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another Methdology is Batch Normalization were we normalize the input before each layer "
      ],
      "metadata": {
        "id": "ff0ld8dzR40Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNN(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.fc1=nn.Linear(784,256)\n",
        "        self.bn1=nn.BatchNorm1d(256)\n",
        "        self.fc2=nn.Linear(256,128)\n",
        "        self.bn2=nn.BatchNorm1d(128)\n",
        "        self.fc3=nn.Linear(128,64)\n",
        "        self.bn3=nn.BatchNorm1d(64)\n",
        "        self.fc4=nn.Linear(64,32)\n",
        "        self.bn4=nn.BatchNorm1d(32)\n",
        "        self.fc5=nn.Linear(32,10)\n",
        "  def forward(self,x):\n",
        "        out=torch.sigmoid(self.fc1(x))\n",
        "        out=self.bn1(out)\n",
        "        out=torch.sigmoid(self.fc2(out))\n",
        "        out=self.bn2(out)\n",
        "        out=torch.sigmoid(self.fc3(out))\n",
        "        out=self.bn3(out)\n",
        "        out=torch.sigmoid(self.fc4(out))\n",
        "        out=self.bn4(out)\n",
        "        out=torch.sigmoid(self.fc5(out))\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "uAJV2KX9TVTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "deep_NN = DeepNN()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(deep_NN.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = deep_NN(x)\n",
        "    if outputs is not None:\n",
        "        loss = criterion(outputs, torch.argmax(y, dim=1))\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Print gradients at every 100 epochs\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
        "            \n",
        "\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkeOcxWrT7FY",
        "outputId": "dacc296c-8c0c-4dd3-9d30-c8a823bb57ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 1.8967\n",
            "Epoch [200/1000], Loss: 1.8420\n",
            "Epoch [300/1000], Loss: 1.8162\n",
            "Epoch [400/1000], Loss: 1.7984\n",
            "Epoch [500/1000], Loss: 1.7842\n",
            "Epoch [600/1000], Loss: 1.7720\n",
            "Epoch [700/1000], Loss: 1.7611\n",
            "Epoch [800/1000], Loss: 1.7511\n",
            "Epoch [900/1000], Loss: 1.7418\n",
            "Epoch [1000/1000], Loss: 1.7329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using RELU as our activation function rather than sigmoid to avoid the saturation we encounter leading to vanishing gradient."
      ],
      "metadata": {
        "id": "0Fb09TbrUHN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNN(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.fc1=nn.Linear(784,256)\n",
        "        self.fc2=nn.Linear(256,128)\n",
        "        self.fc3=nn.Linear(128,64)\n",
        "        self.fc4=nn.Linear(64,32)\n",
        "        self.fc5=nn.Linear(32,10)\n",
        "  def forward(self,x):\n",
        "        out=torch.relu(self.fc1(x))\n",
        "        out=torch.relu(self.fc2(out))\n",
        "        out=torch.relu(self.fc3(out))\n",
        "        out=torch.relu(self.fc4(out))\n",
        "        out=torch.relu(self.fc5(out))\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "Xpv_y2VyT8eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "deep_NN = DeepNN()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(deep_NN.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = deep_NN(x)\n",
        "    if outputs is not None:\n",
        "        loss = criterion(outputs, torch.argmax(y, dim=1))\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Print gradients at every 100 epochs\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
        "            \n",
        "\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBXu6NH8UWby",
        "outputId": "b60f22df-f77a-417d-b288-9a815d22da82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 2.2460\n",
            "Epoch [200/1000], Loss: 2.1930\n",
            "Epoch [300/1000], Loss: 2.1227\n",
            "Epoch [400/1000], Loss: 1.9790\n",
            "Epoch [500/1000], Loss: 1.7709\n",
            "Epoch [600/1000], Loss: 1.6776\n",
            "Epoch [700/1000], Loss: 1.6253\n",
            "Epoch [800/1000], Loss: 1.5610\n",
            "Epoch [900/1000], Loss: 1.4665\n",
            "Epoch [1000/1000], Loss: 1.3859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8DYrRJMUXtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploding Gradient "
      ],
      "metadata": {
        "id": "fdJNtOB0V6Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNN(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.fc1=nn.Linear(784,256)\n",
        "        self.fc2=nn.Linear(256,512)\n",
        "        self.fc3=nn.Linear(512,64)\n",
        "        self.fc4=nn.Linear(64,32)\n",
        "        self.fc5=nn.Linear(32,10)\n",
        "\n",
        "# Initialize large weights for each layer \n",
        "        nn.init.normal_(self.fc1.weight, mean=0, std=10)\n",
        "        nn.init.normal_(self.fc2.weight, mean=0, std=10)\n",
        "        nn.init.normal_(self.fc3.weight, mean=0, std=10)\n",
        "        nn.init.normal_(self.fc4.weight, mean=0, std=10)\n",
        "        nn.init.normal_(self.fc5.weight, mean=0, std=10)\n",
        "\n",
        "  def forward(self,x):\n",
        "        out=torch.relu(self.fc1(x))\n",
        "        out=torch.relu(self.fc2(out))\n",
        "        out=torch.relu(self.fc3(out))\n",
        "        out=torch.relu(self.fc4(out))\n",
        "        out=torch.relu(self.fc5(out))\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "HOnNHUOwV9OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "deep_NN = DeepNN()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(deep_NN.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = deep_NN(x)\n",
        "    if outputs is not None:\n",
        "        loss = criterion(outputs, torch.argmax(y, dim=1))\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Print gradients at every 100 epochs\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
        "            \n",
        "\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6vbga-uV-rj",
        "outputId": "8999e6c3-2e55-4401-8715-e2dfc87e2fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: nan\n",
            "Epoch [200/1000], Loss: nan\n",
            "Epoch [300/1000], Loss: nan\n",
            "Epoch [400/1000], Loss: nan\n",
            "Epoch [500/1000], Loss: nan\n",
            "Epoch [600/1000], Loss: nan\n",
            "Epoch [700/1000], Loss: nan\n",
            "Epoch [800/1000], Loss: nan\n",
            "Epoch [900/1000], Loss: nan\n",
            "Epoch [1000/1000], Loss: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNN(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.fc1=nn.Linear(784,256)\n",
        "        self.fc2=nn.Linear(256,512)\n",
        "        self.fc3=nn.Linear(512,64)\n",
        "        self.fc4=nn.Linear(64,32)\n",
        "        self.fc5=nn.Linear(32,10)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "        out=torch.sigmoid(self.fc1(x))\n",
        "        out=torch.sigmoid(self.fc2(out))\n",
        "        out=torch.sigmoid(self.fc3(out))\n",
        "        out=torch.sigmoid(self.fc4(out))\n",
        "        out=torch.sigmoid(self.fc5(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "c0qq7hI50r40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep=DeepNN()\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(deep.parameters(), lr=0.01)\n",
        "\n",
        "# Define the gradient clip value\n",
        "clip_value = 1\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = deep(x)\n",
        "    if outputs is not None:\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, torch.argmax(y, dim=1))\n",
        "\n",
        "        # Backward pass and gradient clipping\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(deep.parameters(), clip_value) # Apply gradient clipping\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print the loss\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbBEJ86HWbOS",
        "outputId": "f867a5fd-153e-45ad-e44d-5d0d1fea9e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 2.2872\n",
            "Epoch [200/1000], Loss: 2.2693\n",
            "Epoch [300/1000], Loss: 2.2552\n",
            "Epoch [400/1000], Loss: 2.2442\n",
            "Epoch [500/1000], Loss: 2.2356\n",
            "Epoch [600/1000], Loss: 2.2286\n",
            "Epoch [700/1000], Loss: 2.2229\n",
            "Epoch [800/1000], Loss: 2.2181\n",
            "Epoch [900/1000], Loss: 2.2140\n",
            "Epoch [1000/1000], Loss: 2.2105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nOFFYOkwkl2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RESNET(Residual Network)\n",
        "\n"
      ],
      "metadata": {
        "id": "tcETWzKJ16vC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define a Plain CNN"
      ],
      "metadata": {
        "id": "9zvyWoiwGM48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IMPORT LIBARARIES"
      ],
      "metadata": {
        "id": "CwklnRAPGQr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "nlgRA2cDVRhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Simple CNN\n",
        "\n",
        "\n",
        "\n",
        "- Number of Convolutional Layers: 4\n",
        "- Number of Convolutional Filters: 64, 128, 256, 512\n",
        "- Filter/Kernel Size: 7x7, 3x3, 3x3, 3x3\n",
        "- Padding: Same padding (pad with zeros to maintain input size) for all convolutional layers\n",
        "- Stride: 2 for the first convolutional layer, 1 for the rest\n",
        "- Activation Function: Sigmoid for all convolutional layers\n",
        "- Number of Fully Connected Layers: 1\n",
        "- Number of Hidden Units in Fully Connected Layer: 512\n",
        "- Output Layer: Fully Connected Layer with number of units equal to the number of classes in the dataset (e.g., 10 for FashionMNIST)\n",
        "- Activation Function in Output Layer: None\n"
      ],
      "metadata": {
        "id": "MfyVLlqIVT85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the PlainNet model\n",
        "class PlainNet(nn.Module):\n",
        "    def __init__(self, image_channels, num_classes):\n",
        "        super(PlainNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = nn.functional.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = nn.functional.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = nn.functional.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = nn.functional.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Csq-DOaKVXdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOAD FashionMNIST dataset"
      ],
      "metadata": {
        "id": "7Sq_ocgpVcHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the FashionMNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "train_dataset = FashionMNIST(root='.', train=True, transform=transform, download=True)\n",
        "test_dataset = FashionMNIST(root='.', train=False, transform=transform, download=True)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "c6DyfR46VhBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24d4d6e-35d7-4bcf-f81d-f5e89989b5f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 12075941.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/raw/train-images-idx3-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 199810.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3704038.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 11844364.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the model"
      ],
      "metadata": {
        "id": "3nunaT7YVp_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PlainNet model\n",
        "image_channels = 1 #Gray scale\n",
        "num_classes = 10\n",
        "model = PlainNet(image_channels, num_classes)\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzY7t5XiGLr4",
        "outputId": "c4490294-1e55-4399-80f2-fb78ae9961b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/938], Loss: 2.300379991531372\n",
            "Epoch [1/10], Step [200/938], Loss: 2.3279833793640137\n",
            "Epoch [1/10], Step [300/938], Loss: 2.3913798332214355\n",
            "Epoch [1/10], Step [400/938], Loss: 2.330376625061035\n",
            "Epoch [1/10], Step [500/938], Loss: 2.3539109230041504\n",
            "Epoch [1/10], Step [600/938], Loss: 2.3022844791412354\n",
            "Epoch [1/10], Step [700/938], Loss: 2.2668755054473877\n",
            "Epoch [1/10], Step [800/938], Loss: 2.288836717605591\n",
            "Epoch [1/10], Step [900/938], Loss: 2.3341193199157715\n",
            "Epoch [2/10], Step [100/938], Loss: 2.328723430633545\n",
            "Epoch [2/10], Step [200/938], Loss: 2.3523526191711426\n",
            "Epoch [2/10], Step [300/938], Loss: 2.3069543838500977\n",
            "Epoch [2/10], Step [400/938], Loss: 2.280301809310913\n",
            "Epoch [2/10], Step [500/938], Loss: 2.319952964782715\n",
            "Epoch [2/10], Step [600/938], Loss: 2.3494131565093994\n",
            "Epoch [2/10], Step [700/938], Loss: 2.302398443222046\n",
            "Epoch [2/10], Step [800/938], Loss: 2.3101301193237305\n",
            "Epoch [2/10], Step [900/938], Loss: 2.2949931621551514\n",
            "Epoch [3/10], Step [100/938], Loss: 2.328047513961792\n",
            "Epoch [3/10], Step [200/938], Loss: 2.2993180751800537\n",
            "Epoch [3/10], Step [300/938], Loss: 2.3364181518554688\n",
            "Epoch [3/10], Step [400/938], Loss: 2.3072948455810547\n",
            "Epoch [3/10], Step [500/938], Loss: 2.2955050468444824\n",
            "Epoch [3/10], Step [600/938], Loss: 2.3177192211151123\n",
            "Epoch [3/10], Step [700/938], Loss: 2.3063106536865234\n",
            "Epoch [3/10], Step [800/938], Loss: 2.3023483753204346\n",
            "Epoch [3/10], Step [900/938], Loss: 2.3063032627105713\n",
            "Epoch [4/10], Step [100/938], Loss: 2.304624557495117\n",
            "Epoch [4/10], Step [200/938], Loss: 2.2991487979888916\n",
            "Epoch [4/10], Step [300/938], Loss: 2.3108537197113037\n",
            "Epoch [4/10], Step [400/938], Loss: 2.3062243461608887\n",
            "Epoch [4/10], Step [500/938], Loss: 2.322199821472168\n",
            "Epoch [4/10], Step [600/938], Loss: 2.292132616043091\n",
            "Epoch [4/10], Step [700/938], Loss: 2.3056604862213135\n",
            "Epoch [4/10], Step [800/938], Loss: 2.3169357776641846\n",
            "Epoch [4/10], Step [900/938], Loss: 2.2925047874450684\n",
            "Epoch [5/10], Step [100/938], Loss: 2.296168088912964\n",
            "Epoch [5/10], Step [200/938], Loss: 2.3315274715423584\n",
            "Epoch [5/10], Step [300/938], Loss: 2.3050575256347656\n",
            "Epoch [5/10], Step [400/938], Loss: 2.3030593395233154\n",
            "Epoch [5/10], Step [500/938], Loss: 2.2946557998657227\n",
            "Epoch [5/10], Step [600/938], Loss: 2.311882257461548\n",
            "Epoch [5/10], Step [700/938], Loss: 2.301283836364746\n",
            "Epoch [5/10], Step [800/938], Loss: 2.3167829513549805\n",
            "Epoch [5/10], Step [900/938], Loss: 2.304438352584839\n",
            "Epoch [6/10], Step [100/938], Loss: 2.3050312995910645\n",
            "Epoch [6/10], Step [200/938], Loss: 2.3095898628234863\n",
            "Epoch [6/10], Step [300/938], Loss: 2.3118789196014404\n",
            "Epoch [6/10], Step [400/938], Loss: 2.3074276447296143\n",
            "Epoch [6/10], Step [500/938], Loss: 2.300333261489868\n",
            "Epoch [6/10], Step [600/938], Loss: 2.2944486141204834\n",
            "Epoch [6/10], Step [700/938], Loss: 2.3207123279571533\n",
            "Epoch [6/10], Step [800/938], Loss: 2.2893805503845215\n",
            "Epoch [6/10], Step [900/938], Loss: 2.3052244186401367\n",
            "Epoch [7/10], Step [100/938], Loss: 2.304171323776245\n",
            "Epoch [7/10], Step [200/938], Loss: 2.291142463684082\n",
            "Epoch [7/10], Step [300/938], Loss: 2.3038880825042725\n",
            "Epoch [7/10], Step [400/938], Loss: 2.3070902824401855\n",
            "Epoch [7/10], Step [500/938], Loss: 2.308974266052246\n",
            "Epoch [7/10], Step [600/938], Loss: 2.307260036468506\n",
            "Epoch [7/10], Step [700/938], Loss: 2.306302070617676\n",
            "Epoch [7/10], Step [800/938], Loss: 2.2986457347869873\n",
            "Epoch [7/10], Step [900/938], Loss: 2.2859625816345215\n",
            "Epoch [8/10], Step [100/938], Loss: 2.306919574737549\n",
            "Epoch [8/10], Step [200/938], Loss: 2.3046748638153076\n",
            "Epoch [8/10], Step [300/938], Loss: 2.314716339111328\n",
            "Epoch [8/10], Step [400/938], Loss: 2.2971997261047363\n",
            "Epoch [8/10], Step [500/938], Loss: 2.304792642593384\n",
            "Epoch [8/10], Step [600/938], Loss: 2.3061435222625732\n",
            "Epoch [8/10], Step [700/938], Loss: 2.2987732887268066\n",
            "Epoch [8/10], Step [800/938], Loss: 2.3004326820373535\n",
            "Epoch [8/10], Step [900/938], Loss: 2.2985177040100098\n",
            "Epoch [9/10], Step [100/938], Loss: 2.3077328205108643\n",
            "Epoch [9/10], Step [200/938], Loss: 2.3286244869232178\n",
            "Epoch [9/10], Step [300/938], Loss: 2.300374984741211\n",
            "Epoch [9/10], Step [400/938], Loss: 2.3010663986206055\n",
            "Epoch [9/10], Step [500/938], Loss: 2.3032007217407227\n",
            "Epoch [9/10], Step [600/938], Loss: 2.3063697814941406\n",
            "Epoch [9/10], Step [700/938], Loss: 2.3022472858428955\n",
            "Epoch [9/10], Step [800/938], Loss: 2.2939560413360596\n",
            "Epoch [9/10], Step [900/938], Loss: 2.301002025604248\n",
            "Epoch [10/10], Step [100/938], Loss: 2.301999807357788\n",
            "Epoch [10/10], Step [200/938], Loss: 2.3001365661621094\n",
            "Epoch [10/10], Step [300/938], Loss: 2.305952787399292\n",
            "Epoch [10/10], Step [400/938], Loss: 2.310336112976074\n",
            "Epoch [10/10], Step [500/938], Loss: 2.3087236881256104\n",
            "Epoch [10/10], Step [600/938], Loss: 2.3047330379486084\n",
            "Epoch [10/10], Step [700/938], Loss: 2.313638210296631\n",
            "Epoch [10/10], Step [800/938], Loss: 2.301542282104492\n",
            "Epoch [10/10], Step [900/938], Loss: 2.295257329940796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the test images: {:.2f} %'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCEweDCoVNWL",
        "outputId": "e6f8adc0-867c-46a9-a4a2-82db8fdc6fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the test images: 10.00 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RESNET Architecture\n",
        "#Residual Net:\n",
        "\n",
        "- Number of Convolutional Layers: 4\n",
        "- Number of Convolutional Filters: 64,64,128,256\n",
        "-Filter/Kernel Size: 7x7, 3x3, 3x3, 3x3\n",
        "-Padding: Same padding (pad with zeros to maintain input size)\n",
        "-Activation Function: ReLU\n",
        "-Number of Residual Blocks: 3\n",
        "-Number of Fully Connected Layers: 1\n",
        "-Number of Hidden Units in Fully Connected Layer: 512\n",
        "-Output Layer: Fully Connected Layer with number of units equal to the number of classes in the dataset (e.g., 10 for FashionMNIST)\n",
        "-Activation Function in Output Layer: None\n",
        "#Residual Blocks:\n",
        "\n",
        "- Number of Convolutional Layers in each Residual Block: 2\n",
        "- Number of Convolutional Filters in each Residual Block: 64\n",
        "- Filter/Kernel Size in each Convolutional Layer: 3x3\n",
        "-Padding: Same padding (pad with zeros to maintain input size)\n",
        "- Activation Function: ReLU"
      ],
      "metadata": {
        "id": "0O-NB95BV6L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ResidualBlock module\n",
        "class ReBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ReBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x += self.shortcut(residual)\n",
        "        x = self.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kfjG7b3eWjAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ResNet model\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(64, 64, 2)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride=1):\n",
        "        layers = []\n",
        "        for _ in range(num_blocks):\n",
        "            layers.append(ReBlock(in_channels, out_channels, stride))\n",
        "            in_channels = out_channels\n",
        "            stride = 1\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "wFfwYsu1V96C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Model"
      ],
      "metadata": {
        "id": "PKdWg8yMW70n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ResNet18 model\n",
        "image_channels = 1 #Grayscale\n",
        "num_classes = 10 \n",
        "model = ResNet(image_channels, num_classes)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Move model to GPU \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch + 1, num_epochs, i + 1, len(train_loader), loss.item()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46NbL7SOGUXS",
        "outputId": "8b1ebfbd-dd6c-487c-aff1-4684a7a6ce25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/938], Loss: 0.7295\n",
            "Epoch [1/5], Step [200/938], Loss: 0.4479\n",
            "Epoch [1/5], Step [300/938], Loss: 0.4427\n",
            "Epoch [1/5], Step [400/938], Loss: 0.3131\n",
            "Epoch [1/5], Step [500/938], Loss: 0.3766\n",
            "Epoch [1/5], Step [600/938], Loss: 0.3161\n",
            "Epoch [1/5], Step [700/938], Loss: 0.2819\n",
            "Epoch [1/5], Step [800/938], Loss: 0.3777\n",
            "Epoch [1/5], Step [900/938], Loss: 0.5203\n",
            "Epoch [2/5], Step [100/938], Loss: 0.2652\n",
            "Epoch [2/5], Step [200/938], Loss: 0.3453\n",
            "Epoch [2/5], Step [300/938], Loss: 0.3226\n",
            "Epoch [2/5], Step [400/938], Loss: 0.3404\n",
            "Epoch [2/5], Step [500/938], Loss: 0.2602\n",
            "Epoch [2/5], Step [600/938], Loss: 0.4311\n",
            "Epoch [2/5], Step [700/938], Loss: 0.4720\n",
            "Epoch [2/5], Step [800/938], Loss: 0.1982\n",
            "Epoch [2/5], Step [900/938], Loss: 0.3398\n",
            "Epoch [3/5], Step [100/938], Loss: 0.3248\n",
            "Epoch [3/5], Step [200/938], Loss: 0.3955\n",
            "Epoch [3/5], Step [300/938], Loss: 0.2315\n",
            "Epoch [3/5], Step [400/938], Loss: 0.3655\n",
            "Epoch [3/5], Step [500/938], Loss: 0.3716\n",
            "Epoch [3/5], Step [600/938], Loss: 0.2577\n",
            "Epoch [3/5], Step [700/938], Loss: 0.2844\n",
            "Epoch [3/5], Step [800/938], Loss: 0.2183\n",
            "Epoch [3/5], Step [900/938], Loss: 0.4838\n",
            "Epoch [4/5], Step [100/938], Loss: 0.1841\n",
            "Epoch [4/5], Step [200/938], Loss: 0.1630\n",
            "Epoch [4/5], Step [300/938], Loss: 0.1534\n",
            "Epoch [4/5], Step [400/938], Loss: 0.1945\n",
            "Epoch [4/5], Step [500/938], Loss: 0.2792\n",
            "Epoch [4/5], Step [600/938], Loss: 0.2236\n",
            "Epoch [4/5], Step [700/938], Loss: 0.3412\n",
            "Epoch [4/5], Step [800/938], Loss: 0.1975\n",
            "Epoch [4/5], Step [900/938], Loss: 0.1806\n",
            "Epoch [5/5], Step [100/938], Loss: 0.1950\n",
            "Epoch [5/5], Step [200/938], Loss: 0.2336\n",
            "Epoch [5/5], Step [300/938], Loss: 0.3336\n",
            "Epoch [5/5], Step [400/938], Loss: 0.1954\n",
            "Epoch [5/5], Step [500/938], Loss: 0.2569\n",
            "Epoch [5/5], Step [600/938], Loss: 0.3090\n",
            "Epoch [5/5], Step [700/938], Loss: 0.1858\n",
            "Epoch [5/5], Step [800/938], Loss: 0.1349\n",
            "Epoch [5/5], Step [900/938], Loss: 0.1704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the test images: {:.2f} %'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dcuvd7r4KgfZ",
        "outputId": "bcb7618c-9f05-48c3-8763-3df512c521a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the test images: 90.15 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q_9-7wrG2NAS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}